-Solve xgboost for higgs data set completely with pred file--- done
- Understand higgs data :1. how tree is made and
					2. how variable importance is

Note on spliting test/train: stratification is used to split test and train datasets so that equal proportion of both exists.

parameters:
- lr
- Sampling rate(subset of random records).
- Unique records
- READ REAL PAPER OF xgBOOST
- Grid search algo for hyperparam: takes all the parameters.
- Compare itrations and comp times with hyperopt vs GA  library in conj with grid.(hyperop:bayes thm)
- Use higgs and housing data as well or IRIS data set.
- then explore autoencoders
- Loss function for multi label dataset - hamming distance
- build histogram of video data between the all combinations(111,110,.....000)
- build histo for labels as well
- early stop parameter
- hamming distance
- levenberg and marquadt algohome
- catboost(impr on xgboost).
- dropout in xg boost.
- rapids.ai(gpu data).
- svm is good binary classifier.
- All optimisations in deep nets are first derivative based.
- Sabastian roder book gives understanding of second derivatives.
- Second derivative optimisers in neuro fuzzy model.
- Levenburg markenburg algorithm
- autocoders and elm

SEE: https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/ for specifics on package installation
